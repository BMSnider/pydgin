#!/usr/bin/env python
#=========================================================================
# pareto-improved [options]
#=========================================================================
#
#  -h --help           Display this message
#
#   --g_ncores         Number of cores
#   --g_insn_ports     Number of instruction ports
#   --g_resources      Number of backend resources
#   --runtime          Select runtime
#                       [spmd,wsrt]
#  --component        Select component to be used for stack bar plots
#
# Author : Shreesha Srinath
# Date   : March 11th, 2018
#
# Script that is useful for pareto analysis. Dumps a csv that can be sliced
# and diced in useful ways for visualization
#

import argparse
import re
import math
import sys

import pandas as pd
pd.set_option('display.width', 1000)

from scipy.stats.mstats import gmean

from collections import Counter

from common import *
from common_configs import *

#-------------------------------------------------------------------------
# Command line processing
#-------------------------------------------------------------------------

class ArgumentParserWithCustomError(argparse.ArgumentParser):
  def error( self, msg = "" ):
    if ( msg ): print("\n ERROR: %s" % msg)
    print("")
    file = open( sys.argv[0] )
    for ( lineno, line ) in enumerate( file ):
      if ( line[0] != '#' ): sys.exit(msg != "")
      if ( (lineno == 2) or (lineno >= 4) ): print( line[1:].rstrip("\n") )

def parse_cmdline():
  p = ArgumentParserWithCustomError( add_help=False )

  # Standard command line arguments

  p.add_argument( "-h", "--help",    action="store_true" )

  # Additional commane line arguments for the simulator

  p.add_argument( "--basic", action="store_false",  default=True )

  p.add_argument( "--g_ncores",     type=int, action="store", default=4)
  p.add_argument( "--g_insn_ports", type=int, action="store", default=4)
  p.add_argument( "--g_resources",  type=int, action="store", default=4)

  p.add_argument( "--component", action="store", default='mem',
                  choices=["iaccess","daccess","execute","frontend","mem","work"])
  p.add_argument( "--runtime", action="store", default="spmd",
                  choices=["spmd", "wsrt"])

  opts = p.parse_args()
  if opts.help: p.error()
  return opts

#-------------------------------------------------------------------------
# is_pareto_front()
#-------------------------------------------------------------------------
# reference:
# http://hinnefe2.github.io/python/tools/2015/09/21/mario-kart.html

def is_pareto_front(row, stats, xlabel, ylabel):
  x = row[xlabel]
  y = row[ylabel]

  # look for points with the same y value but smaller x value
  is_min_x = stats.loc[stats[ylabel]==y].max()[xlabel] >= x
  # look for points with the same x value but smaller y value
  is_min_y = stats.loc[stats[xlabel]==x].max()[ylabel] >= y

  # look for points that are smaller in both x and y
  is_min_xy = len(stats.loc[(stats[xlabel]<x) & (stats[ylabel]<y)])==0

  return is_min_x and is_min_y and is_min_xy

#-------------------------------------------------------------------------
# normalize results
#-------------------------------------------------------------------------
# computes normalized delay and normalized work for each config

def normalize_results( df ):
  df['normalized_delay'] = df['steps']
  df['normalized_yaxis'] = df[g_unique_label]
  for runtime in ['spmd','wsrt']:
    base_cfg = g_mimd_base_str % runtime
    for app in app_list:
      try:
        base_steps = float(df.loc[(df.app == app) & (df.config == base_cfg), 'steps'].iloc[0])
        df.loc[(df.app==app) & df.config.str.contains(runtime), 'normalized_delay'] = \
          df.loc[(df.app==app) & df.config.str.contains(runtime), 'normalized_delay'] / base_steps
        base_yaxis = float(df.loc[(df.app == app) & (df.config == base_cfg), g_total_label].iloc[0])
        df.loc[(df.app==app) & df.config.str.contains(runtime), 'normalized_yaxis'] = \
          df.loc[(df.app==app) & df.config.str.contains(runtime), 'normalized_yaxis'] * 100/ base_yaxis
      except:
        continue
  return df

#-------------------------------------------------------------------------
# per_app_per_config
#-------------------------------------------------------------------------

def per_app_per_config( df, group_dict, configs_dict ):
  with open( "pareto_configs.csv", "w" ) as out:
    out.write( "runtime,app,group,config,delay,yaxis\n" )
    for runtime in ['spmd','wsrt']:
      rt_df = df.loc[df.config.str.contains(runtime)]
      for app in app_list:
        if app not in rt_df.app.unique():
          continue
        for group, configs_list in group_dict.iteritems():
          data = []
          for cfg in configs_list:
            if runtime not in cfg:
              continue
            try:
              delay = rt_df.loc[(rt_df.app==app) & (rt_df.config==cfg), 'normalized_delay'].iloc[0]
              yaxis = rt_df.loc[(rt_df.app==app) & (rt_df.config==cfg), 'normalized_yaxis'].iloc[0]
              data.append( [cfg, delay, yaxis] )
            except:
              print "Exception: {%s,%s,%s}" % ( group, app, cfg )
              continue

          columns   = ['config','delay','yaxis']
          stats     = pd.DataFrame(data,columns=columns)
          is_pareto = stats.apply(lambda row: is_pareto_front(row, stats, 'delay', 'yaxis'), axis=1)
          
          optimal_configs = stats.loc[is_pareto,'config'].tolist()
          for cfg in optimal_configs:
            delay = stats.loc[(stats.config == cfg), 'delay'].iloc[0]
            yaxis = stats.loc[(stats.config == cfg), 'yaxis'].iloc[0]
            out.write( "%s,%s,%s,%s,%f,%f\n" % ( runtime, app, group, cfg, delay, yaxis ) )

#-------------------------------------------------------------------------
# main
#-------------------------------------------------------------------------

if __name__ == "__main__":

  opts = parse_cmdline()

  # global options
  if   opts.component == "work":
    g_total_label  = "total_work"
    g_unique_label = "unique_work"
    g_ylabel       = "Work"
  elif opts.component == "mem":
    g_total_label  = "total_mem"
    g_unique_label = "unique_mem"
    g_ylabel       = "Mem. Access"
  elif opts.component == "iaccess":
    g_total_label  = "total_iaccess"
    g_unique_label = "unique_iaccess"
    g_ylabel       = "Insn. Access"
  elif opts.component == "daccess":
    g_total_label  = "total_daccess"
    g_unique_label = "unique_daccess"
    g_ylabel       = "Data Access"
  elif opts.component == "frontend":
    g_total_label  = "total_frontend"
    g_unique_label = "unique_frontend"
    g_ylabel       = "Frontend"
  elif opts.component == "execute":
    g_total_label  = "total_execute"
    g_unique_label = "unique_execute"
    g_ylabel       = "Backend"

  # skip limited-lockstep and adaptive_hints
  basic = opts.basic

  # read the results
  df = pd.read_csv( g_sim_results_file )

  # skip non-zero values which indicate missing data
  df = df[df!=0]

  # normalize the results for each runtime
  df = normalize_results( df )

  # get the configs seperated based on the uarch
  group_dict   = populate_configs( basic )
  configs_dict = format_config_names( basic )

  # per app pareto optimal configs
  per_app_per_config( df, group_dict, configs_dict )
